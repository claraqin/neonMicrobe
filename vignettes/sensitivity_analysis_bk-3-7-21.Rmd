
---
title: "Sensitivity Analysis for Quality Filtering Parameters"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sensitivity-analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
devtools::load_all() # TODO: Before publishing, replace this with library(neonMicrobe)
knitr::opts_knit$set(
  root.dir = NEONMICROBE_DIR_BASE()
)
```

The purpose of this vignette is to demonstrate how a researcher can test the effects of various DADA2 quality filtering parameter combinations on the outputs of the DADA2 pipeline, and ultimately, on the ecological inference. Specifically, this vignette asks the following from a subset of NEON 16S marker gene sequences:

* What are the effects of quality filtering parameters on the number of remaining reads?
* What are the effects of quality filtering parameters on alpha and beta diversity metrics?

The parameters that we vary to observe their downstream effects include the following:

* `trunLen.R`: Reverse reads that do not meet or exceed `truncLen.R` in length will be discarded. Reverse reads that exceed `truncLen.R` will be truncated to `truncLen.R`.
* `maxEE.R`: After truncation, reads with greater than `maxEE` expected errors will be discarded. Expected errors are calculated from the nominal definition of the quality score: EE = sum(10^(-Q/10)).

Because we are interested in variation in these benchmark metrics across different parameter sets, we refer to this analysis as a **sensitivity analysis**. This vignette is intended to provide a boilerplate for users of this R package to construct their own sensitivity analyses.


# Dependencies

To begin, ensure that you have installed all dependencies.

Load libraries:

```{r, eval=FALSE}
library(neonMicrobe)
```

```{r message=FALSE}
library(dada2)
library(ShortRead)
library(Biostrings)
library(tibble)
library(dplyr)
library(vegan)
library(phyloseq)
library(ggplot2)
library(tidyr)
```

# Set up base directory

Setting the base directory using `setBaseDirectory()` will create a point of reference for the `neonMicrobe` package, so that it knows where to look for the raw sequence files, and where to save the processed data. This should be the same base directory that you used in the "Download NEON Data" vignette.

```{r}
print(dirname(getwd()))
```

```{r}
setBaseDirectory(dirname(getwd()))
```


## Constants used for this sensitivity analysis:

Quality filter parameters that we do not vary in this sensitivity analysis

```{r}
MAX_EE_FWD <- 8
TRUNC_LEN_FWD <- 240
MIN_LEN <- 50
```

Parameters specific to this sensitivity analysis

```{r}
N_SAMPLES <- 200
DIRNAME_TEST <- "qf_test_1-13-2021"
runIDs <-  c("runB69PP", "runB69RN", "runB9994", "runBDNB6", "runBF462", "runBFDG8", "runBNMJ5", "runBNMWB", "runBRPH4", "runC24VW", "runC25T6", "runC5B2R", "runC7WK3", "runC8VMV", "runC977L", "runC983L", "runCBJYB", "runCBTWG", "runCDHG2", "runCDJ5J")
```

Parameter value grid. The following allows testing of two quality filtering parameters at a time.

```{r}
PARAM1 <- "maxEE.R"
PARAM2 <- "truncLen.R"
grid <- expand.grid(c(4, 8, 16), c(170, 220, 250))
params <- matrix(
  c(grid[,1],     # PARAM1
    grid[,2]),    # PARAM2
  byrow=FALSE, ncol=2,
  dimnames = list(NULL, c(PARAM1, PARAM2))
)
param_sets <- apply(params, 1, function(x) paste(c(rbind(c(PARAM1, PARAM2), x)), collapse="_"))
```

# Assign filepath variables

Special directories were used for this sensitivity analysis.

```{r}
PATH_16S <- file.path(NEONMICROBE_DIR_SEQUENCE(), "16S")
# PATH_RAW <- file.path(PATH_16S, "0_raw")
PATH_TRIMMED <- file.path(PATH_16S, "1_trimmed")
PATH_TEST <- file.path(PATH_16S, DIRNAME_TEST)
dir.create(file.path(PATH_TEST, "results"), recursive=TRUE)
```

## Get fastq files for this analysis

Retrieve files that match the sequence run IDs specified earlier.

```{r}
rawFs <- sort(list.files(PATH_RAW, pattern = paste0("(",paste0("(", runIDs, ")", collapse="|"),")", ".*_R1\\.fastq"), full.names = TRUE))
rawRs <- sort(list.files(PATH_RAW, pattern = paste0("(",paste0("(", runIDs, ")", collapse="|"),")", ".*_R2\\.fastq"), full.names = TRUE))
```


Retrieve metadata. If you saved your metadata to file, you can load it here:

```{r}
meta <- read.csv("./data/sequence_metadata/mmg_soilMetadata_16S_2021-01-12135435.csv")
```

If you did not save your metadata to file, you can re-download it using `downloadSequenceMetadata()`:

```{r eval=FALSE}
meta <- downloadSequenceMetadata(targetGene = "16S")
```

Remove any samples that only have forward reads or only have reverse reads.

```{r}
matched_fn <- removeUnmatchedFastqFiles(rawFs, rawRs, meta)
rawFs <- matched_fn[[1]]
rawRs <- matched_fn[[2]]
```

To cut down on computation time, select up to N_SAMPLES samples from the runs, up to N_SAMPLES/length(runIDs) from each run.

```{r}
if(length(rawFs) > N_SAMPLES) {
  rawFs_subset <- c()
  rawRs_subset <- c()
  for(i in 1:length(runIDs)) {
    rawFs_runID <- rawFs[grep(runIDs[i], rawFs)]
    rawRs_runID <- rawRs[grep(runIDs[i], rawRs)]
    if(length(rawFs_runID) > N_SAMPLES/length(runIDs)) {
      set.seed(101010+i)
      subset <- sample(seq(1,length(rawFs_runID)), N_SAMPLES/length(runIDs), FALSE)
      rawFs_subset <- c(rawFs_subset, rawFs_runID[subset])
      rawRs_subset <- c(rawRs_subset, rawRs_runID[subset])
    } else {
      rawFs_subset <- c(rawFs_subset, rawFs_runID)
      rawRs_subset <- c(rawRs_subset, rawRs_runID)
    }
  }
  rawFs <- rawFs_subset
  rawRs <- rawRs_subset
}
write.csv(cbind(rawFs, rawRs), file.path(PATH_TEST, "input_files.csv"))
```

Ensure there are no poor quality flags or legacy data in the subset.

```{r}
meta_ext <- matchFastqToMetadata(c(rawFs, rawRs), meta)
with(meta_ext, table(sequencerRunID, qaqcStatus.dna))
with(meta_ext, table(sequencerRunID, qaqcStatus.pcr))
with(meta_ext, table(sequencerRunID, qaqcStatus.seq))
with(meta_ext, table(sequencerRunID, dataQF.rawFiles))
with(meta_ext, table(sequencerRunID, dataQF.dna))
with(meta_ext, table(sequencerRunID, dataQF.pcr))
with(meta_ext, table(sequencerRunID, dataQF.seq))
```

Plot quality profiles

```{r fig.show="hold", out.width="25%"}
profiles_list <- list()
for(i in 1:length(runIDs)) {
  # Retrieve only those files associated with the appropriate runID
  profiles_list[[i]] <- gridExtra::grid.arrange(
    plotQualityProfile(grep(runIDs[i], rawFs, value=TRUE), aggregate=TRUE),
    plotQualityProfile(grep(runIDs[i], rawRs, value=TRUE), aggregate=TRUE),
    ncol=2,
    top=runIDs[i]
  )
}
for(i in 1:length(profiles_list)) {
  plot(profiles_list[[i]])
}
```


Split complete filenames into "basenames" and directory names

```{r}
fn_base <- basename(c(rawFs, rawRs))
PATH_PARAMSETS <- file.path(PATH_TEST, param_sets)
```


## Trim primers from sequences

Trim reads based on the primer sequences supplied in `params.R`.

```{r, eval=FALSE}
trim_trackReads <- trimPrimers16S(fn_base, in_subdir="raw", out_explicitdir=PATH_TRIMMED,
                                  primer_16S_fwd="CCTACGGGNBGCASCAG", primer_16S_rev="GACTACNVGGGTATCTAATCC")
# trim_trackReads <- trimPrimers16S(fn_base, PATH_RAW, PATH_TRIMMED, "CCTACGGGNBGCASCAG", "GACTACNVGGGTATCTAATCC")
```

## Run quality filter on sequences

To store the results arising from each set of parameter choices, we construct a list object where each element corresponds to the output given a different parameter set. For this step, the list object is `filter_trackReads`.

```{r, eval=FALSE}
filter_trackReads <- list()
for(i in 1:length(param_sets)) {
  filter_trackReads[[i]] <- qualityFilter16S(
    fn_base,
    in_explicitdir = PATH_TRIMMED,
    out_explicitdir = PATH_PARAMSETS[[i]],
    maxEE=c(MAX_EE_FWD, params[i,1]), # Vary maxEE.R
    truncLen=c(TRUNC_LEN_FWD, params[i,2]), # Vary truncLen.R
    minLen=MIN_LEN,
    multithread=TRUE) # set FALSE for Windows computers
  rownames(filter_trackReads[[i]]) <- paste0(param_sets[i], "|", rownames(filter_trackReads[[i]]))
}
filter_trackReads_mat <- do.call(rbind, filter_trackReads)
```

Optionally, save the read-tracking table, in case the job fails:

```{r, eval=FALSE}
write.csv(filter_trackReads_mat, file.path(PATH_TEST, "results",  "sensitivity_trackReads_filter.csv"), row.names=TRUE)
```

## Run the rest of the processing pipeline

Although the quality filtering step is the only part of the processing pipeline where we vary the parameters, we must follow through with the rest of the pipeline to observe the downstream effects on remaining reads, merging rate, taxonomic resolution, and alpha- and beta-diversity estimates.

This requires us to redefine the list structure so that it becomes nested: in the first level, each element corresponds to a parameter set; in the second level, each element corresponds to a sequencing run ID. This is necessary because the dada sequence inference algorithm is sensitive to error rate estimates, and error rates may differ considerably between sequencing runs. Here we initialize the nested list structure for two types of outputs simultaneously.

```{r}
seqtabs <- dada_trackReads <- lapply(1:length(param_sets), function(x) lapply(1:length(runIDs), function(y) list()))
```

Now we continue with the processing pipeline to produce different versions of the resulting sequence tables.

```{r}
for(i in 1:length(param_sets)) {
  for(j in 1:length(runIDs)) {
    message("Sensitivity analysis: parameter set ", param_sets[i], ", sequencing run ",  runIDs[j])

    # Retrieve only those files associated with the appropriate parameter set and runID
    fn_base.star <- grep(runIDs[j], fn_base, value=TRUE)

    seqtab.list <- runDada16S(fn_base.star, PATH_PARAMSETS[i], MULTITHREAD, VERBOSE, seed=11001100)
    seqtabs[[i]][[j]] <- seqtab.list$seqtab.nochim
    dada_trackReads[[i]][[j]] <- seqtab.list$track

    rownames(seqtabs[[i]][[j]]) <- paste0(param_sets[i], "|", rownames(seqtabs[[i]][[j]]))
    rownames(dada_trackReads[[i]][[j]]) <- paste0(param_sets[i], "|", rownames(dada_trackReads[[i]][[j]]))
    
    # Can save work as you go:
    saveRDS(seqtab.list$seqtab.nochim, file.path(PATH_PARAMSETS[i], paste0("sensitivity_seqtab_", runIDs[j], ".Rds")))
    write.csv(seqtab.list$track, file.path(PATH_PARAMSETS[i], paste0("sensitivity_trackReads_dada_", runIDs[j], ".csv")), row.names=TRUE)
  }
}

dada_trackReads_mat <- do.call(rbind, lapply(dada_trackReads, function(x) do.call(rbind, x)))
```

Combine all read-tracking tables:

```{r}
trim_trackReads_mat <- do.call(rbind, lapply(1:length(param_sets), function(i) {
  rownames(trim_trackReads) <- paste0(param_sets[i], "|", rownames(trim_trackReads))
  trim_trackReads
}))

track <- Reduce(
  function(x, y, ...) transform(merge(x, y, by=0, all = TRUE, ...), row.names=Row.names, Row.names=NULL),
  list(trim_trackReads_mat[,1],
       filter_trackReads_mat,
       dada_trackReads_mat)
)
names(track) <- c("input", "trimmed", "filtered", colnames(dada_trackReads_mat))
track[is.na(track)] <- 0

```

Now that sequence inference is complete, we can join the sequencing runs back together in each parameter set:

```{r}
seqtabs_joinrun <- lapply(1:length(param_sets), function(x) list())
for(i in 1:length(param_sets)) {
  seqtabs_joinrun[[i]] <- mergeSequenceTables(tables = seqtabs[[i]])
}
```

Save the data so far:

```{r}
saveRDS(seqtabs_joinrun, file.path(PATH_TEST, "results", "sensitivity_seqtabs_joinrun_list.Rds"))
write.csv(track, file.path(PATH_TEST, "results", "sensitivity_trackReads.csv"), row.names=TRUE)
```

You can reload RDS objects into R to pick up where you left off:

```{r, eval=FALSE}
seqtabs_joinrun <- readRDS(file.path(PATH_16S, "qf_test_1-13-2021", "results", "sensitivity_seqtabs_joinrun_list.Rds"))
track <- read.csv(file.path(PATH_16S, "qf_test_1-13-2021", "results", "sensitivity_trackReads.csv"), row.names=1)
```


## Sensitivity of read counts

We can now plot the number of reads remaining after each step in the processing pipeline.

First, add parameter information to the read tracking table. This custom function simply parses the parameter sets from the rownames of the tracking table.

```{r}
track <- parseParamsFromRownames(track, PARAM1, PARAM2)
```

Next, perform a few more operations to make the tracking table ready for plotting:

```{r}
# Reshape read tracking table
track_long <- tidyr::gather(track, key = "step", value = "reads", input:nonchim)
# Exclude metrics associated only with forward reads
track_long <- track_long[!grepl("F$", track_long$step),]
# Aggregate read counts by run ID
track_long[["step"]] <- factor(track_long[["step"]], levels=colnames(track)[1:9])
track_aggRun <- group_by(track_long, maxEE.R, truncLen.R, runID, step) %>%
  dplyr::summarise(reads = sum(reads))
```

Plot!

```{r, fig.width=9, fig.height=3.5}
theme_set(theme_bw())
ggplot(track_aggRun, aes(x=step, y=reads, col=as.factor(truncLen.R))) +
  geom_line(aes(linetype=as.factor(maxEE.R), group=interaction(maxEE.R, truncLen.R))) +
  # facet_wrap(~runID, scales="free_y") +
  facet_wrap(~runID) +
  labs(linetype="maxEE.R", color="truncLen.R") +
  scale_linetype_manual(values=c("dotted", "dashed", "solid")) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  scale_y_continuous(trans="log10")
ggsave(file.path(PATH_TEST, "results", "track_reads_plot.png"), width=10, height=7, units="in")
```


## Sensitivity of alpha diversity

First, we convert the sequence tables into phyloseq objects.

```{r}
physeqs <- list() # Create a list of physeq objects
for(i in 1:length(param_sets)) {
  # Sample data (parameters)
  sampledata <- parseParamsFromRownames(seqtabs_joinrun[[i]], PARAM1, PARAM2, keep_rownames=TRUE, keep_original_cols = FALSE)
  physeqs[[i]] <- phyloseq(otu_table(seqtabs_joinrun[[i]], taxa_are_rows=FALSE),
                           sample_data(sampledata))
}
```

We make use of `phyloseq`'s `estimate_richness()` function. 

```{r}
obsrich_list <- shannon_list <- list()
for(i in 1:length(physeqs)) {
  div <- suppressWarnings(
    estimate_richness(physeqs[[i]], measures=c("Observed","Shannon"), split=TRUE)
  )
  obsrich_list[[i]] <- div[,1]
  shannon_list[[i]] <- div[,2]
}

diversity_list <- lapply(seq_along(physeqs), function(i) { 
  parseParamsFromRownames(
    cbind(
      sample_data(physeqs[[i]]), 
      obsrich = obsrich_list[[i]], 
      shannon = shannon_list[[i]]
    ),
    PARAM1, PARAM2
  )
})

diversity_df <- do.call(rbind, diversity_list)
```

Plot!

```{r, fig.width=5, fig.height=5}
theme_set(theme_bw())
ggplot(diversity_df, aes(y=obsrich, col=as.factor(truncLen.R), x=as.factor(maxEE.R))) +
  geom_boxplot() +
  # facet_wrap(~runID, scales="free_y") +
  facet_wrap(~runID) +
  labs(col="truncLen.R", x="maxEE.R") +
  ylab("Observed richness")
ggsave(file.path(PATH_TEST, "results", "diversity_plot_obsrich.png"), width=10, height=7, units="in")
```

```{r, fig.width=5, fig.height=5}
theme_set(theme_bw())
ggplot(diversity_df, aes(y=shannon, col=as.factor(truncLen.R), x=as.factor(maxEE.R))) +
  geom_boxplot() +
  # facet_wrap(~runID, scales="free_y") +
  facet_wrap(~runID) +
  labs(col="truncLen.R", x="maxEE.R") +
  ylab("Shannon index")
ggsave(file.path(PATH_TEST, "results", "diversity_plot_shannon.png"), width=10, height=7, units="in")
```

Test statistical significance using ANOVA

```{r}
obsrich_aov <- aov(log(obsrich) ~ truncLen.R + maxEE.R + runID + truncLen.R:runID + maxEE.R:runID, data=diversity_df)

png(file.path(PATH_TEST, "results", "aov_diagnostics_obsrich.png"))
par(mfrow=c(2,2))
plot(obsrich_aov)
dev.off()
```


```{r}
options(knitr.kable.NA = '')
knitr::kable(summary(obsrich_aov)[[1]], digits=3)
```


```{r}
shannon_aov <- aov(shannon ~ truncLen.R + maxEE.R + runID + truncLen.R:runID + maxEE.R:runID, data=diversity_df)

png(file.path(PATH_TEST, "results", "aov_diagnostics_shannon.png"))
par(mfrow=c(2,2))
plot(shannon_aov)
dev.off()
```

```{r}
options(knitr.kable.NA = '')
knitr::kable(summary(shannon_aov)[[1]], digits=3)
```

**In conclusion,** `truncLen.R` affects alpha-diversity inference, whereas `maxEE.R` does not (within the range of `maxEE.R` values that we tested). `truncLen.R` affects alpha-diversity inference to a different degree depending on the sequence run. 


## Sensitivity of beta diversity

First, combine all sequence tables into ONE large sequence table. A sample data table will distinguish samples processed using different parameters.

```{r}
seqtab_joined <- mergeSequenceTables(tables=seqtabs_joinrun)
saveRDS(seqtab_joined, file.path(PATH_TEST, "results", "seqtab_joined_runs_and_params.Rds"))
#seqtab_joined <- readRDS(file.path(PATH_TEST, "results", "seqtab_joined_runs_and_params.Rds"))
```

Unite sequence-length variants:

```{r, eval=FALSE}
t1 <- Sys.time()
seqtab_joined_collapsed <- collapseNoMismatch(seqtab_joined)
t2 <- Sys.time()
t2-t1
```

Or load the collapsed version of the joined sequence table (recommended!)

```{r}
seqtab_joined <- readRDS("./data/seqtab_joined_runs_and_params_COLLAPSED.Rds")
```

```{r}
# Sample data (parameters)
sampledata <- parseParamsFromRownames(seqtab_joined, PARAM1, PARAM2, keep_rownames=TRUE, keep_original_cols = FALSE)
```

**FOR TESTING ONLY**: Custom function to match rownames in this sequence table (which are based on the original fastq filenames) to the sequence metadata.

```{r}
matchSeqtabToMetadata <- function(tab, meta, verbose=TRUE) {
  rownms <- rownames(tab)
  samplenms <- sub(".*\\|", "", rownms)
  
  # Remove runID if appended to beginning of filename
  key <- sub("^run[A-Za-z0-9]*_", "", samplenms)
  
  # Append "_R1" to end of samplenames
  key <- paste0(key, "_R1.fastq.gz")

  # # Append ".gz" to end of filename if missing
  # key[!grepl(".gz$", key)] <- paste0(key[!grepl(".gz$", key)], ".gz")

  key_match <- match(key, as.character(meta$rawDataFileName))
  if(any(is.na(key_match))) {
    if(verbose) {
      message("Matching file names to metadata: ", sum(is.na(key_match)), " files did not have matching records in the provided metadata. ",
              "Double-check to ensure that the provided metadata is of the appropriate scope.")
    }
  }
  return(cbind(rowname = rownms, meta[key_match,], stringsAsFactors=FALSE))
}

meta_seqtab <- matchSeqtabToMetadata(seqtab_joined, read.csv("./NEON/sequence_metadata/mmg_soilMetadata_16S_2021-01-12135435.csv"))
sampledata_with_meta <- cbind(sampledata, meta_seqtab)
all(rownames(sampledata_with_meta) == meta_seqtab$rowname)
```

```{r}
meta_seqtab %>%
  group_by(dnaSampleID, siteID, plotID, collectDate) %>%
  dplyr::summarise(n=n(), .groups="drop") %>%
  dplyr::select(-n) ->
  meta_seqtab_summary
meta_seqtab_summary
```

Combine sequence table and sample data table into a phyloseq object:

```{r}
physeq_joined <- phyloseq(otu_table(seqtab_joined,taxa_are_rows=FALSE),
                          sample_data(sampledata_with_meta))
```

Remove samples with zero total counts

```{r}
physeq_joined_nonzero <- prune_samples(sample_sums(physeq_joined) > 0, physeq_joined)
```

Optionally, discard taxa with 10 or fewer reads:

```{r}
physeq_joined_nonzero <- prune_taxa(taxa_sums(physeq_joined_nonzero) > 10, physeq_joined_nonzero)
```

Ordinate, one sequencing run at a time. (Samples from different sequencing runs may be highly dissimilar, producing difficult-to-interpret ordination plots.)

```{r, message=FALSE}
ord_list <- ps_runID <- list()

for (i in 1:length(runIDs)) {
  ps_runID[[i]] <- subset_samples(physeq_joined_nonzero, runID==runIDs[i])
  set.seed(1010101)
  ord_list[[i]] <- ordinate(ps_runID[[i]], "NMDS", "bray", k=2)
  saveRDS(ord_list[[i]], file.path(PATH_TEST, "results", paste0("sensitivity_ordination_", runIDs[i], "_COLLAPSED.Rds")))
}
```

Load ordination data if previously run:

```{r}
for(i in 1:length(runIDs)) {
  ord_list[[i]] <- readRDS(file.path(PATH_TEST, "results", paste0("sensitivity_ordination_", runIDs[i], "_COLLAPSED.Rds")))
}
```

Plot the ordinations!

Generalize to plot with one facet for each runID:

```{r}
stress_annotations <- lapply(ord_list, function(o) {
  data.frame(xloc=Inf, yloc=-Inf, 
             label=paste0("Stress: ", formatC(signif(o$stress,digits=3), digits=3,format="fg", flag="#")),
             hjust=1.10, vjust=-0.10)
})

theme_set(theme_bw())
ordplot_list <- list()
for(i in 1:length(ord_list)) {
  sampledata <- as(sample_data(ps_runID[[i]]), "data.frame")
  sampledata$maxEE.R <- as.factor(sampledata$maxEE.R)
  sampledata$truncLen.R <- as.factor(sampledata$truncLen.R)
  sampledata <- cbind(sampledata, scores(ord_list[[i]])[match(rownames(sampledata), rownames(scores(ord_list[[i]]))),])
  ordplot_list[[i]] <- ggplot(sampledata, aes(x=NMDS1, y=NMDS2)) +
    geom_point(aes(col=truncLen.R, shape=maxEE.R)) + 
    stat_ellipse(aes(group=sampleID), lwd=0.1, col="grey20") +
    scale_shape_manual(values=c(21, 22, 24)) +
    guides(col=FALSE, shape=FALSE) +
    ggtitle(runIDs[i]) +
    geom_text(data=stress_annotations[[i]], aes(x=xloc, y=yloc, label=label, hjust=hjust, vjust=vjust), size=3) +
    NULL
}
g <- gridExtra::arrangeGrob(grobs=ordplot_list, ncol=5)
ggsave(file.path(PATH_TEST, "results", "ordination_plot_COLLAPSED.png"), plot=g, width=10, height=7, units="in")
plot(g)
```

Try ordinating ALL sensitivity analysis samples:

```{r}
# Start by pruning low-sequencing depth samples and removing uncommon taxa
ps_nonzero_mindepth <- prune_samples(sample_sums(physeq_joined_nonzero) > 2500, physeq_joined_nonzero)
ps_nonzero_mindepth <- prune_taxa(taxa_sums(ps_nonzero_mindepth) > 15, ps_nonzero_mindepth)
ps_nonzero_mindepth_prop <- transform_sample_counts(ps_nonzero_mindepth, function(x) x/sum(x))
saveRDS(ps_nonzero_mindepth_prop, file.path(PATH_TEST, "results", paste0("sensitivity_ps_nonzero_mindepth_prop_COLLAPSED.Rds")))
```

In this version, we forgot to normalize the sample counts:

```{r, eval=FALSE}
set.seed(1010101)
ord_all <- ordinate(ps_nonzero_mindepth, "NMDS", "bray", k=2)
saveRDS(ord_all, file.path(PATH_TEST, "results", paste0("sensitivity_ordination_all_runs.Rds")))
```

In this version, we normalize by proportionalizing the counts data:

```{r}
set.seed(1010101)
ord_all <- ordinate(ps_nonzero_mindepth_prop, "NMDS", "bray", k=2)
saveRDS(ord_all, file.path(PATH_TEST, "results", paste0("sensitivity_ordination_all_runs_prop_COLLAPSED.Rds")))
```


```{r}
# sampledata <- as(sample_data(ps_nonzero_mindepth), "data.frame")
sampledata <- as(sample_data(ps_nonzero_mindepth_prop), "data.frame")
sampledata$maxEE.R <- as.factor(sampledata$maxEE.R)
sampledata$truncLen.R <- as.factor(sampledata$truncLen.R)
sampledata <- cbind(sampledata, scores(ord_all)[match(rownames(sampledata), rownames(scores(ord_all))),])
sampledata$collectYear <- as.integer(format(as.Date(sampledata$collectDate), "%Y"))
# sampledata$processedYear.rawFiles <- as.integer(format(as.Date(sampledata$processedDate.rawFiles), "%Y"))
# sampledata$processedYear.dna <- as.integer(format(as.Date(sampledata$processedDate.dna), "%Y"))
# sampledata$processedYear.pcr <- as.integer(format(as.Date(sampledata$processedDate.pcr), "%Y"))
# sampledata$processedYear.seq <- as.integer(format(as.Date(sampledata$processedDate.seq), "%Y"))
names(sampledata)
```


```{r, fig.width=12}
theme_set(theme_bw())
ggplot(sampledata, aes(x=NMDS1, y=NMDS2)) + geom_point(aes(col=truncLen.R, shape=maxEE.R)) + stat_ellipse(aes(group=sampleID), alpha=0.3, size=0.2)
```

```{r, fig.width=12}
theme_set(theme_bw())
sampledata %>%
  filter(truncLen.R==220, maxEE.R==4) %>%
  ggplot(aes(x=NMDS1, y=NMDS2)) + geom_point(aes(col=domainID)) + stat_ellipse(aes(col=domainID), alpha=0.5, size=1)
```

Let's try assigning taxonomy

```{r}
seqtab_nonzero_mindepth_prop <- as(otu_table(ps_nonzero_mindepth_prop), "matrix")
t1 <- Sys.time()
tax <- assignTaxonomy(seqtab_nonzero_mindepth_prop, SILVA_REF_PATH, multithread = MULTITHREAD, verbose = VERBOSE)
t2 <- Sys.time()
t2-t1
```
```{r}
tax_table(ps_nonzero_mindepth_prop) <- tax
ps_nonzero_mindepth_prop
sample_data(ps_nonzero_mindepth_prop)$NMDS1 <- scores(ord_all)[,"NMDS1"]
sample_data(ps_nonzero_mindepth_prop)$NMDS2 <- scores(ord_all)[,"NMDS2"]
saveRDS(ps_nonzero_mindepth_prop, file.path(PATH_TEST, "results", "phyloseq_nonzero_mindepth_prop.Rds"))
```

Plot abundances of phyla by sequencing year

```{r}
ps_nonzero_mindepth_prop_phylum <- tax_glom(ps_nonzero_mindepth_prop, taxrank = "Phylum")
plot_bar(ps_nonzero_mindepth_prop_phylum, "Phylum", fill="Phylum", facet_grid = ~ processedYear.seq) + guides(fill=FALSE) + theme(axis.text.x=element_text(angle = -90, vjust = 0.5))
ggsave(file.path(PATH_TEST, "results", "phylum_abund_by_seq_year.png"), width=14, height=4, units="in")
```

The abundance of phyla doesn't seem to change! But maybe it is variation at a finer taxonomic resolution that causes this separation. The following function by [Pauvert](https://gist.github.com/cpauvert/d0d63c0df3a3baa6de00f7e61bcde6ed) can be used to calculate the abundance of the top 20 most abundant taxa, across sequencing years.

```{r}
get_top_taxa<-function(phy, k, by=NULL, species="Species", merge_by_species=FALSE){
  if(merge_by_species){
    phy<-tax_glom(phy, taxrank = species,
                  bad_empty = c(NA, "", " ", "\t", "unclassified","unknown","Unknown","Unclassified"))
    warning("The table was first merged by species.")
    message("Beware that the following species were potentially removed:")
    message("NA, unclassified, unknown, Unknown, Unclassified")
  }
  
  # relative abundance transform
  phy<-transform_sample_counts(phy, function(x) x/sum(x))
  # consider the orientation of the table to average by samples
  if(taxa_are_rows(phy)){
    means<-rowMeans(otu_table(phy))
  } else {
    means<-colMeans(otu_table(phy))
  }
  # Compute the k top average relative abundance
  topk_taxa<-sort(means, decreasing = T)[1:k]
  
  dattot<-data.frame(
    Type="TOTAL",
    Rank=1:k,
    RA=round(100*topk_taxa,1),
    ID=names(topk_taxa),
    stringsAsFactors = F
  )
  dat<-dattot
  
  n_samples<-setNames(nsamples(phy),"TOTAL")
  
  if(!is.null(by)){
    
    phy_merged<-merge_samples(phy, group = by)#Caution, the mean is ignored for otutable. So it is only the sum
    # count how many samples per compartment
    n_compartment<-table(sample_data(phy)[,by])
    # Divide the sum by the total number of samples per compartment
    otu_table(phy_merged)<-otu_table(
      sweep(otu_table(phy_merged),
            MARGIN = 1, # Margin is 1 whatever the orientation
            STATS = n_compartment,
            FUN="/"),
      taxa_are_rows = FALSE)
    
    require(plyr)
    
    dattyp<-ldply(sample_names(phy_merged),function(typ){
      sumtyp<-get_taxa(phy_merged, typ)
      sumtyp<-sumtyp[sumtyp>0]
      ranktyp<-setNames(rank(-sumtyp),
                        names(sumtyp))
      data.frame(
        Type=typ,
        Rank=unname(ranktyp[names(topk_taxa)]),
        RA=unname(round(100*(sumtyp[names(topk_taxa)]/sum(sumtyp)),1)),
        ID=names(topk_taxa),
        stringsAsFactors = F
      )
    })
    dat<-rbind(dattot,dattyp)
    n_samples<-c(n_samples, n_compartment)
  }
  dat.r<-merge(
    reshape2::dcast(dat, ID~Type, value.var = "Rank"),
    reshape2::dcast(dat, ID~Type, value.var = "RA"),
    by="ID",suffixes = c("",".RA"), all=T)
  
  if(!is.null(species)){
    dat.r$Species<-as(tax_table(phy), "matrix")[dat.r$ID,species]
  }
  
  ret.list<-list(
    "top"=dat.r[order(dat.r$TOTAL),],
    "n"=n_samples
  )
  
  return(ret.list)
}

top_taxa <- get_top_taxa(ps_nonzero_mindepth_prop, 20, by="processedYear.seq", species=NULL, merge_by_species=FALSE)
top_taxa$top[,-1]
top_taxa_names <- top_taxa$top[,1]
unname(tax_table(ps_nonzero_mindepth_prop)[top_taxa_names,])
str(top_taxa$top)
top_taxa_summary <- cbind(top_taxa$top[,c("TOTAL", "2017.RA", "2018.RA", "2019.RA", "TOTAL.RA")], tax_table(ps_nonzero_mindepth_prop)[top_taxa_names,], Sequence=top_taxa_names)
rownames(top_taxa_summary) <- NULL
names(top_taxa_summary)[1] <- "Rank"
top_taxa_summary
write.csv(top_taxa_summary, file.path(PATH_TEST, "results", "top_taxa_by_sequencing_year.csv"))
```

```{r}
soildata <- readRDS("/raid/users/claraqin/zhulab/neonSoilMicrobeProcessing/soilSample_data_allsites.rds")
sampledata %>%
  left_join(select(soildata, geneticSampleID, horizon)) ->
  sampledata
mean(is.na(sampledata$horizon)) # 0
sampledata_selectparams <- sampledata[sampledata$maxEE.R==4 & sampledata$truncLen.R==220,]
theme_set(theme_bw())
ggplot(sampledata_selectparams, aes(x=NMDS1, y=NMDS2)) + geom_point(aes(col=horizon))
```

```{r}
sample_data(ps_nonzero_mindepth)$processedYear.seq <- as.integer(format(as.Date(get_variable(ps_nonzero_mindepth, "processedDate.seq")), "%Y"))
ps_seq201718 <- subset_samples(ps_nonzero_mindepth, processedYear.seq %in% c(2017, 2018))
set.seed(1010101)
ord_201718 <- ordinate(ps_seq201718, "NMDS", "bray", k=2)
saveRDS(ord_201718, file.path(PATH_TEST, "results", paste0("sensitivity_ordination_seq2017-18.Rds")))
```


```{r}
sampledata <- as(sample_data(ps_seq201718), "data.frame")
sampledata$maxEE.R <- as.factor(sampledata$maxEE.R)
sampledata$truncLen.R <- as.factor(sampledata$truncLen.R)
sampledata <- cbind(sampledata, scores(ord_all)[match(rownames(sampledata), rownames(scores(ord_all))),])
sampledata$collectYear <- as.integer(format(as.Date(sampledata$collectDate), "%Y"))
sampledata$processedYear.rawFiles <- as.integer(format(as.Date(sampledata$processedDate.rawFiles), "%Y"))
sampledata$processedYear.dna <- as.integer(format(as.Date(sampledata$processedDate.dna), "%Y"))
sampledata$processedYear.pcr <- as.integer(format(as.Date(sampledata$processedDate.pcr), "%Y"))
sampledata$processedYear.seq <- as.integer(format(as.Date(sampledata$processedDate.seq), "%Y"))
```


### Test statistical significance using permANOVA. 

First run permANOVA using all three levels of `truncLen.R`.

```{r}
ps_joined_dist <- vegdist(otu_table(ps_nonzero_mindepth_prop))
```

```{r}
adonis(ps_joined_dist ~ maxEE.R + truncLen.R,
       data = as(sample_data(ps_nonzero_mindepth_prop), "data.frame"),
       strata = get_variable(ps_nonzero_mindepth_prop, "sampleID"),
       permutations=999)
```

Also, use betadisper to ensure homogeneity of within-group variances (dispersion). Heterogeneity of within-group variances may be confounded with distances between groups in the permANOVA test.

```{r}
beta_disper <- vegan::betadisper(ps_joined_dist, get_variable(ps_nonzero_mindepth_prop, "truncLen.R"))
anova(beta_disper)
```

```{r}
mod.HSD <- TukeyHSD(beta_disper)
plot(mod.HSD)
```

(Previous four steps combined)

```{r}
t0 <- Sys.time()
ps_joined_dist <- vegdist(otu_table(ps_nonzero_mindepth_prop)) # Takes about 2.5 hours. Next steps take a few minutes
saveRDS(ps_joined_dist, file.path(PATH_TEST, "results", "sensitivity_dist_nonzero_mindepth_prop_COLLAPSED.Rds"))
t1 <- Sys.time()
adonis(ps_joined_dist ~ maxEE.R + truncLen.R,
       data = as(sample_data(ps_nonzero_mindepth_prop), "data.frame"),
       strata = get_variable(ps_nonzero_mindepth_prop, "sampleID"),
       permutations=999)
t2 <- Sys.time()
beta_disper <- vegan::betadisper(ps_joined_dist, get_variable(ps_nonzero_mindepth_prop, "truncLen.R"))
t3 <- Sys.time()
c(t1, t2, t3) - t0
```

```{r}
perm <- how(nperm=999)
setBlocks(perm) <- get_variable(ps_nonzero_mindepth_prop, "sampleID")
t4 <- Sys.time()
adonis2(ps_joined_dist ~ maxEE.R + truncLen.R,
        data = as(sample_data(ps_nonzero_mindepth_prop), "data.frame"),
        permutations=perm)
t5 <- Sys.time()
t5-t4
```

Next re-run permANOVA using only top two levels of `truncLen.R`.

```{r}
ps_nonzero_mindepth_prop_hitrunc <- subset_samples(ps_nonzero_mindepth_prop, truncLen.R != 170)
t0 <- Sys.time()
ps_joined_dist_hitrunc <- vegdist(otu_table(ps_nonzero_mindepth_prop_hitrunc))
saveRDS(ps_joined_dist_hitrunc, file.path(PATH_TEST, "results", "sensitivity_dist_nonzero_mindepth_prop_hitrunc_COLLAPSED.Rds"))
t1 <- Sys.time()
perm <- how(nperm=999)
setBlocks(perm) <- get_variable(ps_nonzero_mindepth_prop_hitrunc, "sampleID")
adonis2(ps_joined_dist_hitrunc ~ maxEE.R + truncLen.R,
        data = as(sample_data(ps_nonzero_mindepth_prop_hitrunc), "data.frame"),
        permutations=perm)
t2 <- Sys.time()
beta_disper_hitrunc <- vegan::betadisper(ps_joined_dist_hitrunc, get_variable(ps_nonzero_mindepth_prop_hitrunc, "truncLen.R"))
t3 <- Sys.time()
c(t1, t2, t3) - t0
```

(Next blocks are redundant with the previous one.)

```{r}
adonis(ps_joined_dist_hitrunc ~ maxEE.R + truncLen.R,
       data = as(sample_data(physeq_joined_nonzero_runC5B2R_hitrunc), "data.frame"),
       strata = get_variable(physeq_joined_nonzero_runC5B2R_hitrunc, "sampleID"),
       permutations=999)
```

Also, use betadisper to ensure homogeneity of within-group variances (dispersion). Heterogeneity of within-group variances may be confounded with distances between groups in the permANOVA test.

```{r}
beta_disper_hitrunc <- vegan::betadisper(ps_joined_dist_hitrunc, get_variable(physeq_joined_nonzero_runC5B2R_hitrunc, "truncLen.R"))
anova(beta_disper_hitrunc)
```

```{r}
mod.HSD_hitrunc <- TukeyHSD(beta_disper_hitrunc)
plot(mod.HSD_hitrunc)
```

**In conclusion,** both `truncLen.R` and `maxEE.R` affect beta-diversity inference, though to different extents. `truncLen.R` affects mean community composition and within-group variance (dispersion) in a non-linear fashion, with drastically different results for the lowest level of `truncLen.R` tested (`truncLen.R == 160`); This is probably due to the large drop-off in read volume at the merging step due to insufficient read overlap. When ANOVA is constrained to only the higher two levels of `truncLen.R`, both `truncLen.R` and `maxEE.R` are found to be significant in determining mean community composition, but with very small effect sizes.
