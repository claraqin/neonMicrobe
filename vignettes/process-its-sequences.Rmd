---
title: "Process ITS Sequences"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Process ITS Sequences}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
devtools::load_all() # TODO: Switch this to library(neonMicrobe) before publishing
knitr::opts_knit$set(
  root.dir = NEONMICROBE_DIR_BASE()
)
```

This vignette demonstrates how to use the functions and parameters in this package to process the raw NEON ITS sequence data into ASV tables using the DADA2 workflow. This vignette processes only the forward ITS reads, following recommendations by Pauvert et al. (2019).

It is assumed that you have already completed the steps in the "Download NEON Data" vignette, so that there are raw sequence files in the appropriate data directory.

# Load libraries

```{r, eva=FALSE}
library(neonMicrobe)
```

```{r message=FALSE}
library(ShortRead)
library(Biostrings)
library(dada2)
library(dplyr)
```

# Set up base directory

Setting the base directory using `setBaseDirectory()` will create a point of reference for the `neonMicrobe` package, so that it knows where to look for the raw sequence files, and where to save the processed data. This should be the same base directory that you used in the "Download NEON Data" vignette.

```{r}
print(dirname(getwd()))
```

```{r}
setBaseDirectory(getwd())
```

# Setup variables

Get filepath names:

```{r}
if(is.null(PRESET_OUTDIR_SEQUENCE) | PRESET_OUTDIR_SEQUENCE == "") {
  PATH_ITS <- file.path(PRESET_OUTDIR, "raw_sequence", "ITS")
} else {
  PATH_ITS <- file.path(PRESET_OUTDIR, PRESET_OUTDIR_SEQUENCE, "ITS")
}
PATH_RAW <- file.path(PATH_ITS, "0_raw")
PATH_FILTN <- file.path(PATH_ITS, "1_filtN")
PATH_TRIMMED <- file.path(PATH_ITS, "2_trimmed")
PATH_FILTERED <- file.path(PATH_ITS, "3_filtered")
PATH_SEQTABS <- file.path(PATH_ITS, "4_seqtabs")
PATH_TRACK <- file.path(PATH_ITS, "track_reads")
```

Create output directory using today's date:
```{r}
today <- Sys.Date()
PATH_OUTPUTS <- file.path(PRESET_OUTDIR, PRESET_OUTDIR_OUTPUTS, today)
dir.create(PATH_OUTPUTS)
```

Load sequence metadata. We'll pull in the newest metadata file, and give a warning if it doesn't contain as many samples as the sequence table contains.

```{r}
# Find newest metadata file
df <- file.info(list.files(file.path(PRESET_OUTDIR,PRESET_OUTDIR_SEQMETA), full.names = T)) %>% 
  tibble::rownames_to_column('filename') %>%
  filter(grepl("soilMetadata_ITS", filename)) %>%
  arrange(desc(mtime))
selected_metadata <- df[which.max(df$mtime),]$filename
meta_full <- read.csv(selected_metadata)

# n_seq_samples <- nrow(seqtab_orig)
n_sampledata <- nrow(meta_full)
# if (n_seq_samples > n_sampledata) {
# 	msg <- paste0("There appear to be more samples in your sequence table than in your metadata table! You will need to download additional metadata for this vignette to work. See vignette #1 for metadata downloading function.")
# 	message(msg)
# }
message("For ITS processing, used newest sequence metadata file: ", selected_metadata, ". This defines the entire scope of sequence metadata that was processed. If this is not what you wanted, then modify the variable 'select_metadata` and re-run.")
```

Quality control metadata

```{r}
meta <- qcMetadata(meta_full)
meta$date_ymd <- as.Date(format(as.Date(meta$collectDate, format="%Y-%m-%d %H:%M:%S"), "%Y-%m-%d"))
```

Match fastq files to metadata:

```{r}
meta_fn <- matchFastqToMetadata(list.files(PATH_RAW, full.names=TRUE), meta)
```

Get all run IDs so you can process fastq files in sequencing-run batches:

```{r}
unique_runs <- unique(meta_fn$sequencerRunID)
```

# Process reads

We use DADA2 to process reads one sequencing run at a time.

```{r}
seqtab_filenames <- c()
t1 <- Sys.time()
ti <- c()
for (i in 1:length(unique_runs)) {
  runID <- unique_runs[i]
  message(paste0("Began processing ", runID, " at ", Sys.time()))

  # Get fastq files from this sequencing run ID
  fn <- meta_fn$file[which(meta_fn$sequencerRunID==runID)]
  fn_base <- basename(fn)
  if(length(fn)==0) {
    message("No files found for sequencing run ", runID, ". Trying next sequencing run.")
    next
  }
  
  # "Pre-filter" the sequences just to remove those with Ns, but perform no other filtering
  prefilter_trackReads <- qualityFilterITS2(fn, PATH_FILTN, meta, maxN = 0)

  # Trim reads based on the primer lengths supplied in params.r
  trimPrimersITS2(file.path(PATH_FILTN, fn_base), paste0(PATH_TRIMMED,"_mid"), meta, "CTTGGTCATTTAGAGGAAGTAA", "GCTGCGTTCTTCATCGATGC")
  trimPrimersITS2(file.path(paste0(PATH_TRIMMED,"_mid"), fn_base), PATH_TRIMMED, meta, "GCTGCGTTCTTCATCGATGC", "CTTGGTCATTTAGAGGAAGTAA")
  #unlink(paste0(PATH_TRIMMED,"_mid"), recursive=TRUE)
  # Note: We use cutadapt twice because some NEON ITS sequencing runs are in mixed orientation -- that is, some R1 sequences are actually reverse-oriented, and some R2 sequences are actually forward-oriented.

  # Filter reads based on the settings in params.r
  filter_trackReads <- qualityFilterITS2(file.path(PATH_TRIMMED, fn_base), PATH_FILTERED, meta, MULTITHREAD, maxEE=MAX_EE_FWD)

  # Now create sequence table for run
  seqtab.list <- runDadaITS2(file.path(PATH_FILTERED, fn_base), meta, remove_chimeras=TRUE, multithread=MULTITHREAD, verbose=VERBOSE)

  # Create output tracking file
  track <- Reduce(
    function(x, y, ...) transform(merge(x, y, by=0, all = TRUE, ...), row.names=Row.names, Row.names=NULL),
    list(prefilter_trackReads, 
         filter_trackReads, 
         seqtab.list$track)
  )
  names(track)[1:4] <- c("reads.in", "prefiltered", "trimmed", "filtered")
  track[is.na(track)] <- 0

  # Save tracking table (which tracks no. of reads remaining at each stage) and sequence table
  write.csv(track, file.path(PATH_TRACK, paste0("track_reads_",runID,".csv")))
  saveRDS(seqtab.list$seqtab, file.path(PATH_SEQTABS, paste0("NEON_ITS_seqtab_nochim_", runID, ".rds")))
  seqtab_filenames <- c(seqtab_filenames, file.path(PATH_SEQTABS, paste0("NEON_16S_seqtab_nochim_", runID, ".rds")))
  message(paste0("Finished processing reads in ", runID, " at ", Sys.time()))
  message(paste0("Sequencing run-specific sequence tables can be found in ", PATH_SEQTABS))
  
  # Clean up
  rm(prefilter_trackReads)
  rm(filter_trackReads)
  rm(seqtab.list)
}
```

Merge the sequence tables from all runs:

```{r}
if(length(seqtab_filenames) == 1) {
  seqtab_joined <- readRDS(seqtab_filenames[1])
} else {
  seqtab_joined <- mergeSequenceTables(tables = seqtab_filenames)
}
```

Next, we'll assign taxonomy to each of the identified sequences. This is a computationally expensive step - it can take hours, or even days, to run. Because of this, we're going to reduce the size of the dataset, by removing samples with fewer than 1000 reads, as well as any taxa with fewer than 10 counts in the whole dataset. This is optional, and the cutoffs here can be modified.

```{r}
# Remove low-abundance taxa (optional)
keep <- which(colSums(seqtab_orig) > 10)
seqtab_joined <- seqtab_orig[,keep]

# Remove low-quality samples (optional)
keep <- which(rowSums(seqtab) > 1000)
seqtab_joined <- seqtab_joined[keep,]

# Check size of dataset
print(dim(seqtab_joined))
```

## Assign taxonomy using the UNITE reference database

## Download UNITE database if necessary
If you have already downloaded a taxonomic database, change the `UNITE_REF_PATH` parameter in the params.r file.
```{r}
unite.url <- "https://files.plutof.ut.ee/public/orig/E7/28/E728E2CAB797C90A01CD271118F574B8B7D0DAEAB7E81193EB89A2AC769A0896.gz"
if(!file.exists(UNITE_REF_PATH)){
	download.file(unite.url, UNITE_REF_PATH)
}
```

```{r eval=FALSE}
tax <- assignTaxonomy(seqtab_joined, UNITE_REF_PATH, multithread = MULTITHREAD, verbose = VERBOSE)
```

## Save processed data to file
```{r}
# Save OTU table and taxonomic table as RDS files
# to hand off to dada2_to_phyloseq.R
saveRDS(seqtab_joined, file.path(PATH_OUTPUTS, "NEON_ITS_seqtab.rds"))
saveRDS(tax, file.path(PATH_OUTPUTS, "NEON_ITS_tax.rds"))
```

The ASV table and taxonomic table will be joined into a phyloseq object, and will also be joined with environmental data, in the next vignette.
