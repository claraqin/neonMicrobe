---
title: "Process 16S Sequences"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{process-its-sequences}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_knit$set(
  root.dir = rprojroot::find_rstudio_root_file() # Update as necessary. Should refer to the absolute filepath of the project root directory (e.g. .../neonSoilMicrobeProcessing)
)
knitr::opts_knit$set(
  root.dir = "~/Documents/UCSC/Projects/NEON_soil_microbe_processing"
)
```

**Note:** If you are using this vignette as a reference, rather than running it as an RMarkdown script, be aware that you will need to manually set the working directory to the project root directory, e.g. `setwd(".../neonSoilMicrobeProcessing")`.

# Dependencies

To begin, ensure that you have installed all dependencies.

From BiocManager:

```{r eval=FALSE}
if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
BiocManager::install("ShortRead")
BiocManager::install("Biostrings")
BiocManager::install("dada2")
```

From CRAN:
```{r eval=FALSE}
install.packages(dplyr)
```

Load libraries

```{r message=FALSE}
library(ShortRead)
library(Biostrings)
library(dada2)
library(dplyr)
```

In addition, source files associated with this package:

```{r}
source("./R/utils.R")
source("./code/params.R")
```

# Setup variables

Get filepath names:

```{r}
if(is.null(PRESET_OUTDIR_SEQUENCE) | PRESET_OUTDIR_SEQUENCE == "") {
  PATH_16S <- file.path(PRESET_OUTDIR, "raw_sequence", "16S")
} else {
  PATH_16S <- file.path(PRESET_OUTDIR, PRESET_OUTDIR_SEQUENCE, "16S")
}
PATH_RAW <- file.path(PATH_16S, "0_raw")
PATH_TRIMMED <- file.path(PATH_16S, "1_trimmed")
PATH_FILTERED <- file.path(PATH_16S, "2_filtered")
PATH_SEQTABS <- file.path(PATH_16S, "3_seqtabs")
PATH_TRACK <- file.path(PATH_16S, "track_reads")
```

Create output directory using today's date:

```{r}
today <- Sys.Date()
PATH_OUTPUTS <- file.path(PRESET_OUTDIR, PRESET_OUTDIR_OUTPUTS, today)
dir.create(PATH_OUTPUTS)
```

Load sequence metadata. We'll pull in the newest metadata file, and give a warning if it doesn't contain as many samples as the sequence table contains.

```{r}
# Find newest metadata file
df <- file.info(list.files(file.path(PRESET_OUTDIR,PRESET_OUTDIR_SEQMETA), full.names = T)) %>% 
  tibble::rownames_to_column('filename') %>%
  filter(grepl("soilMetadata_16S", filename)) %>%
  arrange(desc(mtime))
selected_metadata <- df[which.max(df$mtime),]$filename
meta_full <- read.csv(selected_metadata)

# n_seq_samples <- nrow(seqtab_orig)
n_sampledata <- nrow(meta_full)
# if (n_seq_samples > n_sampledata) {
# 	msg <- paste0("There appear to be more samples in your sequence table than in your metadata table! You will need to download additional metadata for this vignette to work. See vignette #1 for metadata downloading function.")
# 	message(msg)
# }
message("For 16S processing, used newest sequence metadata file: ", selected_metadata, ". This defines the entire scope of sequence metadata that was processed. If this is not what you wanted, then modify the variable 'select_metadata` and re-run.")
```

Quality control metadata

```{r}
meta <- qcMetadata(meta_full)
meta$date_ymd <- as.Date(format(as.Date(meta$collectDate, format="%Y-%m-%d %H:%M:%S"), "%Y-%m-%d"))
```

Match fastq files to metadata:

```{r}
meta_fn <- matchFastqToMetadata(list.files(PATH_RAW, full.names=TRUE), meta)
```

Get all run IDs so you can process fastq files in sequencing-run batches:

```{r}
unique_runs <- unique(meta_fn$sequencerRunID)
```

# Process reads

We use DADA2 to process reads one sequencing run at a time.

```{r}
t1 <- Sys.time()
ti <- c()
first <- TRUE
for (i in 1:length(unique_runs)) {
  runID <- unique_runs[i]
  message(paste0("Began processing ", runID, " at ", Sys.time()))

  # Get fastq files from this sequencing run ID
  fn <- meta_fn$file[which(meta_fn$sequencerRunID==runID)]
  fn_base <- basename(fn)
  if(length(fn)==0) {
    message("No files found for sequencing run ", runID, ". Trying next sequencing run.")
    next
  }

  # Trim reads based on the primer lengths supplied in params.r
  trim_trackReads <- trimPrimers16S2(fn, PATH_TRIMMED, meta, "CCTACGGGNBGCASCAG", "GACTACNVGGGTATCTAATCC")

  # Filter reads based on the settings in params.r
  filter_trackReads <- qualityFilter16S2(file.path(PATH_TRIMMED, fn_base), PATH_FILTERED, meta, multithread = MULTITHREAD, maxEE = c(MAX_EE_FWD, MAX_EE_REV), truncLen=220)

  # Now create sequence table for run
  seqtab.list <- runDada16S2(file.path(PATH_FILTERED, fn_base), meta, remove_chimeras=TRUE, multithread=MULTITHREAD, verbose=VERBOSE)

  # Create output tracking file
  track <- Reduce(
    function(x, y, ...) transform(merge(x, y, by = 0, all = TRUE, ...), row.names=Row.names, Row.names = NULL),
    list(trim_trackReads, 
         filter_trackReads[,2,drop=FALSE], 
         seqtab.list$track)
  )
  names(track)[3] <- "filtered"
  track[is.na(track)] <- 0

  # Save tracking table (which tracks no. of reads remaining at each stage) and sequence table
  write.csv(track, file.path(PATH_TRACK, paste0("track_reads_", runID, ".csv")))
  saveRDS(seqtab.list$seqtab, file.path(PATH_SEQTABS, paste0("NEON_16S_seqtab_nochim_", runID, ".rds")))
  message(paste0("Finished processing reads in ", runID, " at ", Sys.time()))
  message(paste0("Sequencing run-specific sequence tables can be found in ", PATH_SEQTABS))
}
```

Merge the sequence tables from all runs

```{r}
if(length(unique_runs) == 1) {
  seqtab_joined <- seqtab.list$seqtab
} else {
  seqtab_joined <- mergeSequenceTables(tables = file.path(PATH_SEQTABS, paste0("NEON_16S_seqtab_nochim_", unique_runs, ".rds")))
}
```

Next, we'll assign taxonomy to each of the identified sequences. This is a computationally expensive step - it can take hours, or even days, to run. Because of this, we're going to reduce the size of the dataset, by removing samples with fewer than 1000 reads, as well as any taxa with fewer than 10 counts in the whole dataset. This is optional, and the cutoffs here can be modified.

```{r}
# Remove low-abundance taxa (optional)
keep <- which(colSums(seqtab_orig) > 10)
seqtab_joined <- seqtab_orig[,keep]

# Remove low-quality samples (optional)
keep <- which(rowSums(seqtab) > 1000)
seqtab_joined <- seqtab_joined[keep,]

# Check size of dataset
print(dim(seqtab_joined))
```


## Download Silva reference database if necessary
If you have already downloaded a taxonomic database, change the `SILVA_REF_PATH` parameter in the params.r file.

```{r}
silva.url <- "https://zenodo.org/record/3986799/files/silva_nr99_v138_train_set.fa.gz"
if(!file.exists(SILVA_REF_PATH)){
	download.file(silva.url, SILVA_REF_PATH)
}

```

## Assign taxonomy using the Silva reference database
```{r}
tax <- assignTaxonomy(seqtab_joined, SILVA_REF_PATH, multithread = MULTITHREAD, verbose = VERBOSE)
```

## Save processed data to file

```{r}
# Save OTU table and taxonomic table as RDS files
# to hand off to dada2_to_phyloseq.R
saveRDS(seqtab_joined, file.path(PATH_OUTPUTS, "NEON_16S_seqtab.rds"))
saveRDS(tax, file.path(PATH_OUTPUTS, "NEON_16S_tax.rds"))
```


