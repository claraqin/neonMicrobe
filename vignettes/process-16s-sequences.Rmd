---
title: "Process 16S Sequences"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Process 16S Sequences}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
devtools::load_all() # TODO: Switch this to library(neonMicrobe) before publishing
knitr::opts_knit$set(
  root.dir = NEONMICROBE_DIR_BASE()
)
```

This vignette demonstrates how to use the functions and parameters in this package to process the raw NEON 16S sequence data into ASV tables using the DADA2 workflow.

It is assumed that you have already completed the steps in the "Download NEON Data" vignette, so that there are raw sequence files in the appropriate data directory.

Note that few of the code chunks in this .Rmd file are actually run, because DADA2 functions have relatively long runtimes. Therefore, outputs like the read tracking tables are not displayed.


# Load libraries

```{r, eva=FALSE}
library(neonMicrobe)
```

```{r message=FALSE}
library(ShortRead)
library(Biostrings)
library(dada2)
library(dplyr)
library(ggplot2)
```


# Set up base directory

Setting the base directory using `setBaseDirectory()` will create a point of reference for the `neonMicrobe` package, so that it knows where to look for the raw sequence files, and where to save the processed data. This should be the same base directory that you used in the "Download NEON Data" vignette.

```{r}
print(dirname(getwd()))
```

```{r}
setBaseDirectory(dirname(getwd()))
```


# Select metadata

As explained in the "Download NEON Data" vignette, `neonMicrobe` is a *metadata-first* processing pipeline. So we begin by retrieving sequence metadata. Any fastq files that are represented in the metadata will be processed by the functions in the next section, and any fastq files that are *not* represented in the metadata will be ignored. Therefore, it is recommended that you use the same metadata you used to download the files in the first place, lest you end up with fewer or more processed samples than intended.

```{r}
data("seqmeta_greatplains")
meta <- seqmeta_greatplains
meta$date_ymd <- as.Date(format(as.Date(meta$collectDate, format="%Y-%m-%d %H:%M:%S"), "%Y-%m-%d"))
```


# Process reads into ASV tables

It is generally better to use DADA2 to process one sequencing run at a time, because different sequencing runs may have different error rates, and the dada sequence-calling algorithm assumes homogeneous error rates across input samples. We can retrieve all sequencing run IDs from the metadata:

```{r}
unique_runs <- unique(meta$sequencerRunID)
```

For demonstration purposes, we show how you would process just one sequencing run. In practice, the remainder of this section may be enclosed within a `for` loop that cycles through all sequencing runs in the metadata.

```{r}
meta_onerun <- meta[which(meta$sequencerRunID==unique_runs[1]),]
nrow(meta_onerun)
```

Although you can specify filenames to process, here we'll just use all the filenames specified in this subset of the metadata. (Don't worry; any files that don't exist in the file system will be safely ignored.) Note that these filenames are just basenames, i.e. they contain no directory path information. This is fine, as we will see in the next code chunk.

```{r}
fl_nm <- meta_onerun$rawDataFileName
head(fl_nm)
```

## Trim primers from the 16S sequences

`trimPrimers16S()` trims primer sequences from 16S reads by using the `dada2` function `filterAndTrim()`. `trimPrimers16S()` assumes that each read begins with its full primer sequence and operates by truncating the beginning of each read by the length of its primer sequence. Should this aspect of the NEON 16S sequence data change, this step will need to be modified.

In `trimPrimers16S()`, the argument `in_subdir` is "raw" -- a reserved value that tells the function to look for the input files in a special subdirectory of `NEONMICROBE_DIR_SEQUENCE()`. Any other value passed to `in_subdir` would tell it to look for the input files in the corresponding subdirectory of `NEONMICROBE_DIR_MIDPROCESS()`. To override this behavior and specify an input directory explicitly, pass a directory path to `in_explicitdir`. The same rules apply for `out_subdir` (which can be overridden with `out_explicitdir`), except that it has no specially reserved values.

`trimPrimers16S()` additionally returns an integer matrix denoting the number of reads remaining after primer-trimming for each input file.

```{r, eval=FALSE}
trim_trackReads <- trimPrimers16S(fl_nm, in_subdir = "raw", out_subdir = "1_trimmed", 
                                  meta = seqmeta_greatplains)
```

## Filter the 16S sequences

`qualityFilter16S()` filters 16S reads by their quality scores, also by using `filterAndTrim()`. The argument structure of `qualityFilter16S()` is similar to that of `trimPrimers16S()`, except for its optional arguments related to quality filtering parameters, which are passed to `filterAndTrim()`. You can learn more about these quality filtering parameters at `?dada2::filterAndTrim`. Here are the definitions of some of the most commonly used parameters:

- `truncLen`: Default 0 (no truncation). Truncate reads after `truncLen` bases. Reads shorter than this are discarded.
- `minLen`: Default 20. Remove reads with length less than `minLen`. `minLen` is enforced after trimming and truncation.
- `maxEE`: Default Inf (no EE filtering). After truncation, reads with higher than `maxEE` "expected errors" will be discarded. Expected errors are calculated from the nominal definition of the quality score: `$EE = \sum_{l=1}^{L} 10^{-Q_l/10}$`, where `$L$` is the length of the read.

How do you decide what quality filtering parameters to use? One way is to use the `dada2` function `plotQualityProfile()` to visually inspect the quality profiles of a collection of reads. `neonMicrobe` also offers an extension of `plotQualityProfile()` called `plotEEProfile()`, which translates quality scores of reads into cumulative expected errors. This is particularly useful for understanding how `truncLen` and `maxEE` interact to affect the read retention rate.

```{r, fig.width=6, fig.height=6}
files_to_plot <- list.files(file.path(NEONMICROBE_DIR_SEQUENCE(), "16S"), full.names=TRUE,
                            pattern="fastq")
plotEEProfile(files_to_plot, aggregate=FALSE) 
# set aggregate = TRUE to produce one plot combining all reads across files
```

The reads in these samples are of reasonably high quality, but as is typical with the Illumina MiSeq platform, the reverse reads are lower in quality than the forward reads. We can set different parameters for the forward and reverse reads by entering a vector of two numbers into each of the quality filtering arguments we wish to specify, `c([forward-read value], [reverse-read value])`.

```{r, eval=FALSE}
filter_trackReads <- qualityFilter16S(
  fl_nm, in_subdir = "1_trimmed", out_subdir = "2_filtered",
  meta = seqmeta_greatplains, truncLen = c(250, 230), maxEE = c(2, 8))
```

## Denoise reads using the dada algorithm

`runDada16S()` picks or "denoises" 16S reads to generate an amplicon sequence variant (ASV) table by using the 'dada' algorithm. It first calculates the expected base-calling error rate of the reads, and then uses the 'dada' divisive partitioning algorithm to iteratively separate reads into groups distinguished by their likelihood of referring to the same true sequence, until a threshold is reached. Once finished, the partitions are considered amplicon sequence variants (ASVs), and are used to populate a sequence abundance table. (Read the DADA2 methods paper, [Callahan et al. (2016)](https://doi.org/10.1038/nmeth.3869), for more details.) 

`runDada16s()` is actually a wrapper for four different `dada2` functions that carry out the process just outlined: `learnErrors()`, `derepFastq()`, `dada()`, and `mergePairs()`. Since so many functions are wrapped by `runDada16S()`, it may be worth familiarizing yourself with the source code for `runDada16()`, as well as the documentation for the wrapped functions, to ensure it is behaving as expected.

The arguments to `runDada16S()` follow the same conventions as `trimPrimers16S()` and `qualityFilter16S()` for the input variables, but `runDada16S()` requires output file names instead of directory names. `out_seqtab` is the file path where the ASV table will be written, and `out_track` is the file path where the read tracking table will be written. By default, these refer to special subdirectories in `NEONMICROBE_DIR_MIDPROCESS()` and `NEONMICROBE_DIR_TRACKREADS()`, respectively, and the file-writing behavior can be suppressed by setting the corresponding argument to `FALSE`. These ASV table and read-tracking table will also be returned by this `runDada16S()`, but since `runDada16S()` takes so long to run, forgetting to assign its output to a variable could result in a lot of lost time... hence, these output file name arguments.

```{r, eval=FALSE} 
dada_out <- runDada16S(
  fl_nm, in_subdir = "2_filtered", meta = seqmeta_greatplains, verbose = FALSE,
  multithread = TRUE) # set multithread = FALSE on Windows computers though
```

**And there you have it: You've created an ASV table from the NEON 16S sequences!**


## Combine the read-tracking tables

It can be useful to track the loss of reads at each step in the processing pipeline, as it may allow you to pinpoint where something went wrong. Each of the processing functions shown above (`trimPrimers16S()`, `qualityFilter16S()`, and `runDada16S()`) returns a read-tracking table. The tables can be combined using the following function. Setting `out_file = NULL` tells the function to save the combined read-tracking table in the specialized directory within `NEONMICROBE_DIR_TRACKREADS()` -- a behavior that can be suppressed by setting `out_file = FALSE`.

```{r, eval=FALSE}
combineReadTrackingTables16S(trim_trackReads, filter_trackReads, dada_out$track, out_file = NULL)
```

At this point, it is safe to clear some variables from the R environment, freeing up memory. This is especially important when processing multiple sequencing runs one after another.

```{r, eval=FALSE}
rm(trim_trackReads)
rm(filter_trackReads)
rm(dada_out)
```


## Merging ASV tables from across sequencing runs

If you've processed multiple sequencing runs separately (as you should!), then you will have created multiple ASV tables. You can merge ASV tables together using the `dada2` function `mergeSequenceTables()`.

```{r, eval=FALSE}
seqtab_joined <- mergeSequenceTables(tables = seqtab_filenames)
```

While `mergeSequenceTables()` checks for duplicate samples, it performs no ASV clustering. Clustering may be prudent when joining sequences across sequencing runs, because the 'dada' algorithm could produce an ASV in one run that is identical to an ASV in another run *except that one of the ASVs is one or two base pairs longer*. (Anecdotally, this can result in the detection of a mean effect on community composition where it does not exist!) ASVs that differ only in length can be combined using any "100% clustering" algorithm, and `dada2` provides one through `collapseNoMismatch()`. Be warned, though, that at the time of writing (`dada2` version 1.12.1), `collapseNoMismatch()` is not parallelizable, and does not scale well with an increasing number of ASVs. 

```{r, eval=FALSE}
seqtab_joined_collapsed <- collapseNoMismatch(seqtab_joined)
write.csv(seqtab_joined_collapse, 
          file.path(NEONMICROBE_PATH_MIDPROCESS(), "16S", "4_collapsed",
                    paste0("asv_16s_collapsed_", sub(" ", "_", gsub(":", "", Sys.time())), ".csv")))
```


# Assigning taxonomy

In this final section of the vignette, we assign taxonomy to each of the identified sequences. This is a computationally expensive step - it can take hours, or even days, to run. Because of this, we're going to reduce the size of the dataset, by removing samples with fewer than 1000 reads, as well as any taxa with fewer than 10 counts in the whole dataset. This is optional, and the cutoffs here can be modified.

```{r, eval=FALSE}
# Remove low-abundance taxa and low-quality samples
keep_taxa <- which(colSums(seqtab_joined_collapsed) > 10)
keep_samples <- which(rowSums(seqtab_joined_collapsed) > 1000)
seqtab_joined_collapsed <- seqtab_joined_collapsed[,keep_taxa]
seqtab_joined_collapsed <- seqtab_joined_collapsed[keep_samples,]

# Check size of dataset
print(dim(seqtab_joined_collapsed))
```

We will be assigning taxonomy using the SILVA reference database. Download the SILVA reference database if necessary.

```{r, eval=FALSE}
silva.url <- "https://zenodo.org/record/3986799/files/silva_nr99_v138_train_set.fa.gz"
download.file(silva.url, NEONMICROBE_DIR_TAXREF(), basename(silva.url))
```

Assign taxonomy using the SILVA reference database:

```{r, eval=FALSE}
tax <- assignTaxonomy(
  seqtab_joined_collapsed, file.path(NEONMICROBE_DIR_TAXREF(), basename(silva.url)), 
  verbose = TRUE, multithread = TRUE) # set multithread = FALSE on Windows computers though
```

Save the taxonomy table to file:

```{r, eval=FALSE}
write.csv(tax, 
          file.path(NEONMICROBE_DIR_OUTPUTS(), "16S", "5_tax", 
                    paste0("tax_16s_collapsed_", sub(" ", "_", gsub(":", "", Sys.time())), ".csv")))
```


